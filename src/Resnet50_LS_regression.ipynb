{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e335b615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model created\n",
      "mean score tensor(0.7565) std score tensor(0.1239)\n",
      "train val test 45000 3741 10000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torchvision.models as tv_models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "import builtins\n",
    "\n",
    "# Set all print statements to flush=True\n",
    "original_print = builtins.print\n",
    "def print(*args, **kwargs):\n",
    "    kwargs.setdefault('flush', True)\n",
    "    original_print(*args, **kwargs)\n",
    "\n",
    "mem_scores = [{},{},{}]\n",
    "for i,dataset in enumerate(['train','val','test']):\n",
    "    with open(f'./lamem/splits/{dataset}_1.txt') as f:\n",
    "        for line in f:\n",
    "            image_id, mem_score = line.strip().split(' ')\n",
    "            mem_scores[i][image_id] = float(mem_score)\n",
    "\n",
    "train_mem_scores, val_mem_scores, test_mem_scores = mem_scores\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()        \n",
    "        #use resnet50 as the base model\n",
    "        #self.resnet50 = tv_models.resnet50(pretrained=True) #old Pytorch\n",
    "        self.resnet50 = tv_models.resnet50()\n",
    "        #modified the last layer for binary classification  \n",
    "        self.resnet50.fc=torch.nn.Linear(2048, 1) #new layer\n",
    "    \n",
    "    def forward(self,x):\n",
    "        z = self.resnet50(x)\n",
    "        z = z.view(-1)\n",
    "        return z\n",
    "\n",
    "model = Net()\n",
    "device = 'cuda:0'# if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "print('model created')\n",
    "\n",
    "# mse = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "batch_size = 32\n",
    "version = 0\n",
    "\n",
    "\n",
    "# Calculate mean and standard deviation of training set target values\n",
    "train_mem_scores_values = list(train_mem_scores.values())\n",
    "mean_score = torch.tensor(train_mem_scores_values).mean()\n",
    "std_score = torch.tensor(train_mem_scores_values).std()\n",
    "print('mean score',mean_score,'std score',std_score)\n",
    "\n",
    "# Normalize target values\n",
    "def normalize_scores(scores):\n",
    "    return (scores - mean_score) / std_score\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Define a different loss function (e.g., Huber loss)\n",
    "huber_loss = nn.SmoothL1Loss()\n",
    "\n",
    "def load_and_preprocess_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    return preprocess(image)\n",
    "\n",
    "print('train val test',len(list(train_mem_scores.keys())), len(list(val_mem_scores.keys())), len(list(test_mem_scores.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d008ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0.0%\n",
      "train 7.1%\n",
      "train 14.2%\n",
      "train 21.3%\n",
      "train 28.4%\n",
      "train 35.6%\n",
      "train 42.7%\n",
      "train 49.8%\n",
      "train 56.9%\n",
      "train 64.0%\n",
      "train 71.1%\n",
      "train 78.2%\n",
      "train 85.3%\n",
      "train 92.5%\n",
      "train 99.6%\n",
      "Epoch 1, Average Training Loss: 0.41663518726062637\n",
      "val 0.0%\n",
      "val 86.2%\n",
      "Epoch 1, Average Validation Loss: 0.3985379694093918\n",
      "save best epoch 0 0.3985379694093918\n",
      "train 0.0%\n",
      "train 7.1%\n",
      "train 14.2%\n",
      "train 21.3%\n",
      "train 28.4%\n",
      "train 35.6%\n",
      "train 42.7%\n",
      "train 49.8%\n",
      "train 56.9%\n",
      "train 64.0%\n",
      "train 71.1%\n",
      "train 78.2%\n",
      "train 85.3%\n",
      "train 92.5%\n",
      "train 99.6%\n",
      "Epoch 2, Average Training Loss: 0.3928633587458734\n",
      "val 0.0%\n",
      "val 86.2%\n",
      "Epoch 2, Average Validation Loss: 0.3791220483594927\n",
      "save best epoch 1 0.3791220483594927\n",
      "train 0.0%\n",
      "train 7.1%\n",
      "train 14.2%\n",
      "train 21.3%\n",
      "train 28.4%\n",
      "train 35.6%\n",
      "train 42.7%\n",
      "train 49.8%\n",
      "train 56.9%\n",
      "train 64.0%\n",
      "train 71.1%\n",
      "train 78.2%\n",
      "train 85.3%\n",
      "train 92.5%\n",
      "train 99.6%\n",
      "Epoch 3, Average Training Loss: 0.3777620646450801\n",
      "val 0.0%\n",
      "val 86.2%\n",
      "Epoch 3, Average Validation Loss: 0.37255169913686553\n",
      "save best epoch 2 0.37255169913686553\n",
      "train 0.0%\n",
      "train 7.1%\n",
      "train 14.2%\n",
      "train 21.3%\n",
      "train 28.4%\n",
      "train 35.6%\n",
      "train 42.7%\n",
      "train 49.8%\n",
      "train 56.9%\n",
      "train 64.0%\n",
      "train 71.1%\n",
      "train 78.2%\n",
      "train 85.3%\n",
      "train 92.5%\n",
      "train 99.6%\n",
      "Epoch 4, Average Training Loss: 0.3672416324953775\n",
      "val 0.0%\n",
      "val 86.2%\n",
      "Epoch 4, Average Validation Loss: 0.3579602372543565\n",
      "save best epoch 3 0.3579602372543565\n",
      "train 0.0%\n",
      "train 7.1%\n",
      "train 14.2%\n",
      "train 21.3%\n",
      "train 28.4%\n",
      "train 35.6%\n",
      "train 42.7%\n",
      "train 49.8%\n",
      "train 56.9%\n",
      "train 64.0%\n",
      "train 71.1%\n",
      "train 78.2%\n",
      "train 85.3%\n",
      "train 92.5%\n",
      "train 99.6%\n",
      "Epoch 5, Average Training Loss: 0.3569912918304142\n",
      "val 0.0%\n",
      "val 86.2%\n",
      "Epoch 5, Average Validation Loss: 0.36036053531128787\n",
      "train 0.0%\n",
      "train 7.1%\n",
      "train 14.2%\n",
      "train 21.3%\n",
      "train 28.4%\n",
      "train 35.6%\n",
      "train 42.7%\n",
      "train 49.8%\n",
      "train 56.9%\n",
      "train 64.0%\n",
      "train 71.1%\n",
      "train 78.2%\n",
      "train 85.3%\n",
      "train 92.5%\n",
      "train 99.6%\n",
      "Epoch 6, Average Training Loss: 0.3437332084579963\n",
      "val 0.0%\n",
      "val 86.2%\n",
      "Epoch 6, Average Validation Loss: 0.35572535056492377\n",
      "save best epoch 5 0.35572535056492377\n",
      "train 0.0%\n",
      "train 7.1%\n",
      "train 14.2%\n",
      "train 21.3%\n",
      "train 28.4%\n",
      "train 35.6%\n",
      "train 42.7%\n",
      "train 49.8%\n",
      "train 56.9%\n",
      "train 64.0%\n",
      "train 71.1%\n",
      "train 78.2%\n",
      "train 85.3%\n",
      "train 92.5%\n",
      "train 99.6%\n",
      "Epoch 7, Average Training Loss: 0.32663303420774287\n",
      "val 0.0%\n",
      "val 86.2%\n",
      "Epoch 7, Average Validation Loss: 0.35493821779201773\n",
      "save best epoch 6 0.35493821779201773\n",
      "train 0.0%\n",
      "train 7.1%\n",
      "train 14.2%\n",
      "train 21.3%\n",
      "train 28.4%\n",
      "train 35.6%\n",
      "train 42.7%\n",
      "train 49.8%\n",
      "train 56.9%\n",
      "train 64.0%\n",
      "train 71.1%\n",
      "train 78.2%\n",
      "train 85.3%\n",
      "train 92.5%\n",
      "train 99.6%\n",
      "Epoch 8, Average Training Loss: 0.30470865831351385\n",
      "val 0.0%\n",
      "val 86.2%\n",
      "Epoch 8, Average Validation Loss: 0.3917571367888615\n",
      "train 0.0%\n",
      "train 7.1%\n",
      "train 14.2%\n",
      "train 21.3%\n",
      "train 28.4%\n",
      "train 35.6%\n",
      "train 42.7%\n",
      "train 49.8%\n",
      "train 56.9%\n",
      "train 64.0%\n",
      "train 71.1%\n",
      "train 78.2%\n",
      "train 85.3%\n",
      "train 92.5%\n",
      "train 99.6%\n",
      "Epoch 9, Average Training Loss: 0.2664629182490821\n",
      "val 0.0%\n",
      "val 86.2%\n",
      "Epoch 9, Average Validation Loss: 0.44354180837499685\n",
      "train 0.0%\n",
      "train 7.1%\n",
      "train 14.2%\n",
      "train 21.3%\n",
      "train 28.4%\n",
      "train 35.6%\n",
      "train 42.7%\n",
      "train 49.8%\n",
      "train 56.9%\n",
      "train 64.0%\n",
      "train 71.1%\n",
      "train 78.2%\n",
      "train 85.3%\n",
      "train 92.5%\n",
      "train 99.6%\n",
      "Epoch 10, Average Training Loss: 0.21319461782935842\n",
      "val 0.0%\n",
      "val 86.2%\n",
      "Epoch 10, Average Validation Loss: 0.4904180634124526\n",
      "train 0.0%\n",
      "train 7.1%\n",
      "train 14.2%\n",
      "train 21.3%\n",
      "train 28.4%\n",
      "train 35.6%\n",
      "train 42.7%\n",
      "train 49.8%\n",
      "train 56.9%\n",
      "train 64.0%\n",
      "train 71.1%\n",
      "train 78.2%\n",
      "train 85.3%\n",
      "train 92.5%\n",
      "train 99.6%\n",
      "Epoch 11, Average Training Loss: 0.16409218473728232\n",
      "val 0.0%\n",
      "val 86.2%\n",
      "Epoch 11, Average Validation Loss: 0.43069621355369175\n",
      "train 0.0%\n",
      "train 7.1%\n",
      "train 14.2%\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = 999999\n",
    "train_l = []\n",
    "val_l = []\n",
    "with open(f'log_resnet50_LS_{version}.txt','w') as f:\n",
    "    for epoch in range(100):\n",
    "        # train\n",
    "        model.train()\n",
    "        train_image_ids = list(train_mem_scores.keys())\n",
    "        train_mem_scores_list = list(train_mem_scores.values())\n",
    "        num_train_batches = len(train_image_ids) // batch_size\n",
    "        avg_train_loss = 0\n",
    "        for i in range(num_train_batches):\n",
    "            if i % 100 == 0:\n",
    "                print(f'train {100*i/num_train_batches:.1f}%')\n",
    "                f.write(f'train {100*i/num_train_batches:.1f}%\\n')\n",
    "                f.flush()\n",
    "            batch_ids = train_image_ids[i*batch_size:(i+1)*batch_size]\n",
    "            batch_scores = torch.tensor(train_mem_scores_list[i*batch_size:(i+1)*batch_size], dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "            batch_images = torch.stack([load_and_preprocess_image(f\"./lamem/images/{image_id}\") for image_id in batch_ids]).to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward + backward + optimize\n",
    "            outputs = model(batch_images)\n",
    "            loss = huber_loss(outputs, normalize_scores(batch_scores))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_train_loss += loss.item()\n",
    "    #         break\n",
    "        avg_train_loss /= num_train_batches\n",
    "        train_l.append(avg_train_loss)\n",
    "        print(f'Epoch {epoch + 1}, Average Training Loss: {avg_train_loss}')\n",
    "        f.write(f'Epoch {epoch + 1}, Average Training Loss: {avg_train_loss}\\n')\n",
    "        f.flush()\n",
    "\n",
    "        # val\n",
    "        model.eval()\n",
    "        val_image_ids = list(val_mem_scores.keys())\n",
    "        val_mem_scores_list = list(val_mem_scores.values())\n",
    "        num_val_batches = len(val_image_ids) // batch_size\n",
    "        with torch.no_grad():\n",
    "            avg_val_loss = 0\n",
    "            for i in range(num_val_batches):\n",
    "                if i % 100 == 0:\n",
    "                    print(f'val {100*i/num_val_batches:.1f}%')\n",
    "                    f.write(f'val {100*i/num_val_batches:.1f}%\\n')\n",
    "                    f.flush()\n",
    "                batch_ids = val_image_ids[i*batch_size:(i+1)*batch_size]\n",
    "                batch_scores = torch.tensor(val_mem_scores_list[i*batch_size:(i+1)*batch_size], dtype=torch.float32).to(device)\n",
    "\n",
    "                batch_images = torch.stack([load_and_preprocess_image(f\"./lamem/images/{image_id}\") for image_id in batch_ids]).to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(batch_images)\n",
    "                loss = huber_loss(outputs, normalize_scores(batch_scores))\n",
    "\n",
    "                avg_val_loss += loss.item()\n",
    "    #             break\n",
    "\n",
    "            avg_val_loss = avg_val_loss / num_val_batches\n",
    "            val_l.append(avg_val_loss)\n",
    "            print(f'Epoch {epoch + 1}, Average Validation Loss: {avg_val_loss}')\n",
    "            f.write(f'Epoch {epoch + 1}, Average Validation Loss: {avg_val_loss}\\n')\n",
    "            f.flush()\n",
    "\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                print('save best epoch',epoch,avg_val_loss)\n",
    "                f.write(f'save best epoch {epoch} {avg_val_loss}\\n')\n",
    "                f.flush()\n",
    "                torch.save(model.state_dict(), f'resnet50_LS_{version}.pth')\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eb8564",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (new_env)",
   "language": "python",
   "name": "new_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
