{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f78d6cb",
   "metadata": {},
   "source": [
    "## Initialize VGG-16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91c87ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rossaroni/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/rossaroni/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "# Load pre-trained VGG-16 model\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "# Freeze model parameters\n",
    "for param in vgg16.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Print the model architecture\n",
    "print(vgg16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2332661",
   "metadata": {},
   "source": [
    "## Intialize forward hooks to get each layer activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e55dcd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "activation = {}\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "#         print(f\"{module.__class__.__name__} output shape:\", output.shape)\n",
    "        activation[module] = output.detach()\n",
    "\n",
    "conv_layers = [vgg16.features[0], vgg16.features[2], vgg16.features[5], vgg16.features[7], \n",
    "               vgg16.features[10], vgg16.features[12], vgg16.features[14], vgg16.features[17], \n",
    "               vgg16.features[19], vgg16.features[21], vgg16.features[24], vgg16.features[26], \n",
    "               vgg16.features[28]]\n",
    "fc_layers = [vgg16.classifier[0], vgg16.classifier[3]]\n",
    "\n",
    "# Register hooks for each Conv and FC layer\n",
    "for layer in conv_layers + fc_layers:\n",
    "    layer.register_forward_hook(hook_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc0e158",
   "metadata": {},
   "source": [
    "## Test on a sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b66d11ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class label:  'gondola',\n",
      "Predicted probability: 0.9050655364990234\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load the image\n",
    "image = Image.open(\"./lamem/images/00000002.jpg\")\n",
    "\n",
    "# Preprocess the image\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_tensor = preprocess(image)\n",
    "input_batch = input_tensor.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Move input batch to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_batch = input_batch.to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "vgg16.eval()\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    output = vgg16(input_batch)\n",
    "\n",
    "# Convert output probabilities to predicted class\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "predicted_class = torch.argmax(probabilities)\n",
    "\n",
    "# Load the ImageNet class labels\n",
    "with open(\"./imagenet_class_labels.txt\") as f:  # Replace \"imagenet_classes.txt\" with the file containing class labels\n",
    "    class_labels = [line.strip()[4:] for line in f.readlines()]\n",
    "\n",
    "# Get the predicted class label\n",
    "predicted_label = class_labels[predicted_class]\n",
    "\n",
    "print(\"Predicted class label:\", predicted_label)\n",
    "print(\"Predicted probability:\", probabilities[predicted_class].item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d03effe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " Linear(in_features=25088, out_features=4096, bias=True),\n",
       " Linear(in_features=4096, out_features=4096, bias=True)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(activation.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63a2c9f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1159888.8750), tensor(2089358.1250))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "(activation[list(activation.keys())[0]]).sum(),(activation[list(activation.keys())[1]]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74c7e434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(activation.keys())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcce639d",
   "metadata": {},
   "source": [
    "## Run experiment on memorability test set 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f40fe393",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_mem_scores = {}\n",
    "with open('./lamem/splits/test_1.txt') as f:\n",
    "    for line in f:\n",
    "        image_id, mem_score = line.strip().split(' ')\n",
    "        image_mem_scores[image_id] = float(mem_score)\n",
    "\n",
    "# Load the ImageNet class labels\n",
    "with open(\"./imagenet_class_labels.txt\") as f:  # Replace \"imagenet_classes.txt\" with the file containing class labels\n",
    "    class_labels = [line.strip()[4:] for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bd33c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 %\n",
      "1.0 %\n",
      "2.0 %\n",
      "3.0 %\n",
      "4.0 %\n",
      "5.0 %\n",
      "6.0 %\n",
      "7.0 %\n",
      "8.0 %\n",
      "9.0 %\n",
      "10.0 %\n",
      "11.0 %\n",
      "12.0 %\n",
      "13.0 %\n",
      "14.0 %\n",
      "15.0 %\n",
      "16.0 %\n",
      "17.0 %\n",
      "18.0 %\n",
      "19.0 %\n",
      "20.0 %\n",
      "21.0 %\n",
      "22.0 %\n",
      "23.0 %\n",
      "24.0 %\n",
      "25.0 %\n",
      "26.0 %\n",
      "27.0 %\n",
      "28.0 %\n",
      "29.0 %\n",
      "30.0 %\n",
      "31.0 %\n",
      "32.0 %\n",
      "33.0 %\n",
      "34.0 %\n",
      "35.0 %\n",
      "36.0 %\n",
      "37.0 %\n",
      "38.0 %\n",
      "39.0 %\n",
      "40.0 %\n",
      "41.0 %\n",
      "42.0 %\n",
      "43.0 %\n",
      "44.0 %\n",
      "45.0 %\n",
      "46.0 %\n",
      "47.0 %\n",
      "48.0 %\n",
      "49.0 %\n",
      "50.0 %\n",
      "51.0 %\n",
      "52.0 %\n",
      "53.0 %\n",
      "54.0 %\n",
      "55.0 %\n",
      "56.0 %\n",
      "57.0 %\n",
      "58.0 %\n",
      "59.0 %\n",
      "60.0 %\n",
      "61.0 %\n",
      "62.0 %\n",
      "63.0 %\n",
      "64.0 %\n",
      "65.0 %\n",
      "66.0 %\n",
      "67.0 %\n",
      "68.0 %\n",
      "69.0 %\n",
      "70.0 %\n",
      "71.0 %\n",
      "72.0 %\n",
      "73.0 %\n",
      "74.0 %\n",
      "75.0 %\n",
      "76.0 %\n",
      "77.0 %\n",
      "78.0 %\n",
      "79.0 %\n",
      "80.0 %\n",
      "81.0 %\n",
      "82.0 %\n",
      "83.0 %\n"
     ]
    }
   ],
   "source": [
    "vgg16.eval()\n",
    "predictions = {}\n",
    "layer_ids = ['1.1','1.2','2.1','2.2','3.1','3.2','3.3','4.1','4.2','4.3','5.1','5.2','5.3','6','7']\n",
    "\n",
    "image_layer_responses = {}\n",
    "for i,image_id in enumerate(image_mem_scores):\n",
    "    if i % 100 == 0:\n",
    "        print(100*i/len(image_mem_scores),'%')\n",
    "    activation = {}\n",
    "    layer_response = {}\n",
    "    image = Image.open(f\"./lamem/images/{image_id}\")\n",
    "\n",
    "    # Preprocess\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.Grayscale(num_output_channels=3),  # Ensure the image has three channels\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    input_tensor = preprocess(image)\n",
    "    input_batch = input_tensor.unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = vgg16(input_batch)\n",
    "\n",
    "    probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "    predicted_class = torch.argmax(probabilities)\n",
    "\n",
    "    predicted_label = class_labels[predicted_class]\n",
    "    predictions[image_id] = [predicted_label,probabilities[predicted_class].item()]\n",
    "    \n",
    "    for j,key in enumerate(activation.keys()):\n",
    "        layer_response[layer_ids[j]] = activation[key].sum().item()\n",
    "    image_layer_responses[image_id] = layer_response\n",
    "#     print(layer_response,image_mem_scores[image_id])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abac0844",
   "metadata": {},
   "source": [
    "## Reshape data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c2bed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_distributions = {}\n",
    "for layer_id in layer_ids:\n",
    "    layer_distributions[layer_id] = {'activations':[],'mem_scores':[]}\n",
    "    \n",
    "for image_id, layer_response_dict in image_layer_responses.items():\n",
    "    for layer_id, layer_response in layer_response_dict.items():\n",
    "        layer_distributions[layer_id]['activations'].append(layer_response)\n",
    "        layer_distributions[layer_id]['mem_scores'].append(image_mem_scores[image_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e980b7",
   "metadata": {},
   "source": [
    "## Calculate Pearson correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35619e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "coeffs = []\n",
    "for layer_id, activations_and_mem_scores in layer_distributions.items():\n",
    "    x,y = activations_and_mem_scores['activations'],activations_and_mem_scores['mem_scores']\n",
    "    corr_coefficient, p_value = pearsonr(x,y)\n",
    "    print(layer_id)\n",
    "    print(\"Pearson correlation coefficient:\", corr_coefficient)\n",
    "    print(\"p-value:\", p_value,'\\n')\n",
    "    coeffs.append(corr_coefficient)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f996c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(layer_ids,coeffs,marker='o',color='k')\n",
    "plt.title('VGG-16 layer activation correlation with image memorability')\n",
    "plt.xlabel('Model layer (1-5 conv, 6-7 fc)')\n",
    "plt.ylabel('Pearson correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6523f5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299349ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
